{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471e4da2-6b6f-485e-90fc-e3f375f7ee35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Leveraging Hive with Spark using Python\n",
    "\n",
    "We will see how to use Spark with Hive, particularly:\n",
    "\n",
    "- how to create and use Hive databases\n",
    "- how to create Hive tables\n",
    "- how to load data to Hive tables\n",
    "- how to insert data into Hive tables\n",
    "- how to read data from Hive tables\n",
    "- we will also see how to save data frames to any Hadoop supported file system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094afc88-fe58-4b47-8c5a-602115122dc4",
   "metadata": {},
   "source": [
    "First we need to get a SparkSession with Hive (and AVRO) support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json,col\n",
    "from pyspark.sql.types import *\n",
    "from os.path import abspath\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"movielens\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.2.0\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de607c-1830-4bcb-bc0e-8bd87a6cc040",
   "metadata": {},
   "source": [
    "Now, we can use Hive commands to see databases and tables. \n",
    "\n",
    "Let's show all the existing databases. At this point, we can also compare this to the output of `show databases` in Hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f94722-7bbb-4eaf-bd57-42d1a2e97977",
   "metadata": {},
   "source": [
    "We can see the functions in Spark.SQL using the command below. At the time of this writing, we have about 360 functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f123fde-5898-4fcd-9a55-5f2a0563c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "fncs =  spark.sql('show functions').collect()\n",
    "len(fncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e7789-a1df-47e5-8e13-6e6a2e7a1c8a",
   "metadata": {},
   "source": [
    "Let's see some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f6c0e-516b-405d-b8b6-0d0f5582e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in fncs[150:161]:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337f8ba-21c0-4068-9e34-0abfc1ec316a",
   "metadata": {},
   "source": [
    "By the way, we can see what a function is used for and what the arguments are as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9950d8-54cd-4455-99b7-8261be97bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe function instr\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba9b7f-f327-4541-b19d-31b494cff9cb",
   "metadata": {},
   "source": [
    "Now, let’s download the data. The data we will use is MovieLens 20M Dataset. We will use movies, ratings and tags data sets. \n",
    "\n",
    "> Note: In Jupyter Notebook `!` enables us to use shell commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a191507-43df-461e-abd4-26c0d56a1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the data you would use the following commands:\n",
    "# !wget -P /tmp/ http://files.grouplens.org/datasets/movielens/ml-latest.zip\n",
    "# !unzip /tmp/ml-latest.zip -d /tmp\n",
    "# !mv /tmp/ml-latest .\n",
    "\n",
    "# In our case the data has been downloaded in advance, so let's see it\n",
    "!ls -l ml-latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a19b02-e795-4203-8edd-6053bdc95991",
   "metadata": {},
   "source": [
    "\n",
    "Let's drop the `movies` database (in case it was previously created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('drop database if exists movies cascade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d49a4-eaf6-46e0-9123-b8e7d5ee0d3a",
   "metadata": {},
   "source": [
    "Now we can create the `movies` database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c82c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('create database if not exists movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d169c-afe8-4111-b89c-597f3e2d827d",
   "metadata": {},
   "source": [
    "Now if we show all the existing databases we should see the `movies` database. At this point, we can also compare this to the output of `show databases` in Hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff112919-e3f6-4d17-993e-920b3a03c4f9",
   "metadata": {},
   "source": [
    "Now, let’s create tables: in text file format, in ORC and in AVRO format. But first, we have to make sure we are using the movies database by switching to it using the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c90500-5ed8-4d7c-9576-ca47c75de680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql('use movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f490b6-e775-4e69-90c3-23ba5527bc46",
   "metadata": {},
   "source": [
    "Let's show the tables existing inside the `movies` database. Since we just created the database, it shouldn't contain any tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01856c9-aa58-45fe-bbe6-ecee04390186",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d315054-649c-4f44-986d-a2c629def76c",
   "metadata": {},
   "source": [
    "The movies dataset has movieId, title and genres fields. The rating dataset, on the other hand, as userId, movieID, rating and timestamp fields. Now, let’s create the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30684f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists movies\")\n",
    "# TEXTFILE format\n",
    "spark.sql('''\n",
    "create table movies\n",
    "(\n",
    "    movieId int,\n",
    "    title string,\n",
    "    genres string\n",
    ")\n",
    "row format delimited fields terminated by \",\"\n",
    "stored as TEXTFILE\n",
    "''')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10711c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists ratings\")\n",
    "# ORC format\n",
    "spark.sql(\"\"\"\n",
    "create table ratings\n",
    "(\n",
    "    userId int,\n",
    "    movieId int,\n",
    "    rating float,\n",
    "    timestamp string\n",
    ")\n",
    "stored as ORC\n",
    "\"\"\")                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table if exists genres_by_count\")\n",
    "# AVRO format\n",
    "spark.sql(\"\"\"\n",
    "create table genres_by_count\n",
    "( \n",
    "    genres string,\n",
    "    count int\n",
    ")\n",
    "stored as AVRO\n",
    "\"\"\")                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e775c-1d77-4974-9548-8dae729c5a3f",
   "metadata": {},
   "source": [
    "Now, let’s see if the tables have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef148ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b584c36-a330-420c-add8-eed15a75d748",
   "metadata": {},
   "source": [
    "We see all the tables we created above.\n",
    "We can get information about a table as below. If we do not include formatted or extended in the command, we see only information about the columns. But now, we see even its location, the database and other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe formatted ratings\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5542a6-0fa6-49d9-be6e-bb8d42a6f5d4",
   "metadata": {},
   "source": [
    "Now let’s load data into the movies table. We can load data from a local file system or from any hadoop supported file system. If we are using a hadoop directory, we have to remove local from the command below. Please refer the Hive manual for details. If we are loading it just one time, we do not need to include overwrite. However, if there is possibility that we could run the code more than one time, including overwrite is important not to append the same dataset to the table again and again. Hive does not do any transformation while loading data into tables. Load operations are currently pure copy/move operations that move datafiles into locations corresponding to Hive tables. Hive does some minimal checks to make sure that the files being loaded match the target table. So, pay careful attention to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5133d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we upload to HDFS (and give permissions) we can use this:\n",
    "# df = spark.sql(\"load data inpath 'hdfs://namenode:8020/savas/movies.csv' overwrite into table movies\")\n",
    "\n",
    "# If file is local then we use this:\n",
    "df = spark.sql(\"load data local inpath 'ml-latest/movies.csv' overwrite into table movies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from movies\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c17b86-cebb-40ea-a563-b2ecd71924b3",
   "metadata": {},
   "source": [
    "Rather than loading the data as a bulk, we can pre-process it and create a data frame and insert our data frame into the table. Let’s insert the rating data by first creating a data frame.\n",
    "\n",
    "We can create dataframes in two ways:\n",
    "\n",
    "1. By using the Spark SQL read function such as spark.read.csv, spark.read.json, spark.read.orc, spark.read.avro, spark.rea.parquet, etc.\n",
    "2. By reading it in as an RDD and converting it to a dataframe after pre-processing it\n",
    "\n",
    "Let’s specify schema for the ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "             StructField('userId', IntegerType()),\n",
    "             StructField('movieId', IntegerType()),\n",
    "             StructField('rating', DoubleType()),\n",
    "             StructField('timestamp', StringType())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227ca8c-3ac8-403e-a8d2-0635f5523a37",
   "metadata": {},
   "source": [
    "Now, we can read it in as dataframe using dataframe reader as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90814f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = spark.read.csv(\"ml-latest/ratings.csv\", schema = schema, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23db69-937f-4d8a-a8ed-5280037b1c90",
   "metadata": {},
   "source": [
    "We can see the schema of the dataframe as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6356a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe5a53-0cab-444b-ab21-8ae36f56cd35",
   "metadata": {},
   "source": [
    "We can also display the first five records from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb194740-6498-4106-bf65-365398e38c19",
   "metadata": {},
   "source": [
    "The second option to create a data frame is to read it in as RDD and change it to data frame by using the `toDF` data frame function or `createDataFrame` from `SparkSession`. Remember, we have to use the `Row` function from `pyspark.sql` to use `toDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc65f0-5fe8-4808-9ca4-84af45a7c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "rdd = spark._sc.textFile(\"/opt/workspace/ml-latest/ratings.csv\")\n",
    "header = rdd.first()\n",
    "ratings_df2 = rdd\\\n",
    "    .filter(lambda line: line != header)\\\n",
    "    .map(lambda line: \n",
    "            Row(\n",
    "                userId = int(line.split(\",\")[0]),\n",
    "                movieId = int(line.split(\",\")[1]),\n",
    "                rating = float(line.split(\",\")[2]),\n",
    "                timestamp = line.split(\",\")[3]\n",
    "            )\n",
    ").toDF()\n",
    "\n",
    "ratings_df2.printSchema()\n",
    "ratings_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e088f-a310-4a54-8c64-e0419ab8210c",
   "metadata": {},
   "source": [
    "We can also do as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37280884-d204-4e36-9ba2-8db06e657267",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.filter(lambda line: line != header).map(lambda line:line.split(\",\"))\n",
    "ratings_df2_b = spark.createDataFrame(rdd2, schema = schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae5844-8a76-4756-aeda-54423051afcf",
   "metadata": {},
   "source": [
    "We see the schema and the the first five records from ratings_df and ratings_df2 are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d9277-e17e-4fb5-8fd8-6b01f83f15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df2.printSchema()\n",
    "ratings_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e68a70-3f06-497d-88dd-fe06febe2e29",
   "metadata": {},
   "source": [
    "To insert a dataframe into a Hive table, we have to first create a temporary table as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.createOrReplaceTempView(\"ratings_df_table\")\n",
    "# ratings_df2.createOrReplaceTempView(\"ratings_df_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532597f-0233-4394-a9a6-ce36a3aa9282",
   "metadata": {},
   "source": [
    " Now, let’s insert the data to the ratings Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spark.sql(\"insert overwrite ratings select * from ratings_df_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be11284-71f7-4506-adc9-1e2c31162329",
   "metadata": {},
   "source": [
    "Next, let’s check if the movies and ratings Hive tables have the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade97aef-606f-4451-9fdd-9fb1dfd7c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "    select 'movies' as tbl, count(*) as cnt from movies\n",
    "    UNION ALL\n",
    "    select 'ratings' as tbl, count(*) as cnt from ratings\n",
    "\"\"\"\n",
    ").show()\n",
    "\n",
    "spark.sql(\"select * from movies limit 10\").show(truncate = False)\n",
    "spark.sql(\"select * from ratings limit 10\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25193c93",
   "metadata": {},
   "source": [
    "We see that we can put our data in Hive tables by either directly loading data in a local or hadoop file system or by creating a data frame and registering the data frame as a temporary table.\n",
    "\n",
    "We can also query data in Hive table and save it another Hive table. Let’s calculate a number of movies by genres and insert those genres which occur more than 500 times to genres_by_count AVRO Hive table we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9797136",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    genres, \n",
    "    count(*) as count \n",
    "from movies\n",
    "group by genres\n",
    "having count(*) > 500 \n",
    "order by count desc\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "insert into table genres_by_count\n",
    "select genres, count(*) as count from movies\n",
    "group by genres\n",
    "having count(*) >= 500\n",
    "order by count desc\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52504f11",
   "metadata": {},
   "source": [
    "Now, we can check if the data has been inserted to the Hive table appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a549a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from genres_by_count order by count desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065e3e4-ef41-4424-b146-4031ef81fa08",
   "metadata": {},
   "source": [
    "We can also use data in Hive tables with other data frames by first registering the data frames as temporary tables.\n",
    "Now, let’s create a temporary table from the tags dataset and then we will join it with movies and rating tables which are in Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "             StructField('userId', IntegerType()),\n",
    "             StructField('movieId', IntegerType()),\n",
    "             StructField('tag', StringType()),\n",
    "             StructField('timestamp', StringType())\n",
    "            ])\n",
    "\n",
    "tags_df = spark.read.csv(\"/opt/workspace/ml-latest/tags.csv\", schema = schema, header = True)\n",
    "tags_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba65789",
   "metadata": {},
   "source": [
    "Next, register the dataframe as temporary table.tags_df.registerTempTable('tags_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df.registerTempTable('tags_df_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccd747",
   "metadata": {},
   "source": [
    "From the show tables Hive command below, we see that three of them are permanent but two of them are temporary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bd6fc",
   "metadata": {},
   "source": [
    "Now, lets’ join the three tables by using inner join. The result is a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c36800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = spark.sql(\n",
    "\"\"\"\n",
    "    select \n",
    "        m.title, \n",
    "        m.genres, \n",
    "        r.movieId, \n",
    "        r.userId, \n",
    "        r.rating, \n",
    "        r.timestamp as ratingTimestamp,\n",
    "        t.tag, \n",
    "        t.timestamp as tagTimestamp \n",
    "    from ratings as r \n",
    "    inner join tags_df_table as t\n",
    "        on r.movieId = t.movieId and r.userId = t.userId \n",
    "    inner join movies as m \n",
    "        on r.movieId = m.movieId\n",
    "\"\"\")\n",
    "\n",
    "type(joined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16fabc",
   "metadata": {},
   "source": [
    "We can see the first five records as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "joined.select(['title','genres','rating']).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3c803",
   "metadata": {},
   "source": [
    "We can also save our dataframe in another file system.\n",
    "Let’s create a new directory and save the dataframe in csv, json, orc and parquet formats.\n",
    "Let’s see two ways to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pwd\n",
    "!rm -rf output\n",
    "!mkdir output\n",
    "joined.write.csv(\"/opt/workspace/output/joined.csv\", header = True)\n",
    "joined.write.json(\"/opt/workspace/output/joined.json\")\n",
    "joined.write.orc(\"/opt/workspace/output/joined_orc\")\n",
    "joined.write.parquet(\"/opt/workspace/output/joined_parquet\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2547e3-4b4f-4283-b953-be26d3591e39",
   "metadata": {},
   "source": [
    "Now, let’s check if the data is there in the formats we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419192f-3a62-434b-992e-e548fb29a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb788aa6-bdc3-4969-bfd8-9b0f628846e5",
   "metadata": {},
   "source": [
    "The second option to save data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00bc82-ef6f-4ee4-ad38-48af9b4c4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.write.format('csv').save(\"/opt/workspace/output/joined2.csv\" , header = True)\n",
    "joined.write.format('json').save(\"/opt/workspace/output/joined2.json\" )\n",
    "joined.write.format('orc').save(\"/opt/workspace/output/joined2_orc\" )\n",
    "joined.write.format('parquet').save(\"/opt/workspace/output/joined2_parquet\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8eb7e-db0e-4a62-91a3-a79003c9a09e",
   "metadata": {},
   "source": [
    "Now, let’s see if we have data from both options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384771d-5343-4e53-b663-c09a508485be",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ce4fd-a8b6-4451-8984-e17327a1b748",
   "metadata": {},
   "source": [
    "Similarly, let’s see two ways to read the data.\n",
    "First option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bf85a-bd6d-4267-9cab-0e389fe14bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv = spark.read.csv('/opt/workspace/output/joined.csv', header = True)\n",
    "read_orc = spark.read.orc('/opt/workspace/output/joined_orc')\n",
    "read_parquet = spark.read.parquet('/opt/workspace/output/joined_parquet')\n",
    "read_orc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa11bee-ae62-4d17-81ec-634dee571934",
   "metadata": {},
   "source": [
    "second option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65c0d2-772b-486f-8e08-b96bcdb3001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "read2_csv = spark.read.format('csv').load('/opt/workspace/output/joined.csv', header = True)\n",
    "read2_orc = spark.read.format('orc').load('/opt/workspace/output/joined_orc')\n",
    "read2_parquet = spark.read.format('parquet').load('/opt/workspace/output/joined_parquet')\n",
    "read2_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60a19b-e0da-46e9-b40d-ef55a1fd886a",
   "metadata": {},
   "source": [
    "We can also write a data frame into a Hive table by using insertInto. This requires that the schema of the DataFrame is the same as the schema of the table.\n",
    "Let’s see the schema of the joined dataframe and create two Hive tables: one in ORC and one in PARQUET formats to insert the dataframe into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2cbe39-b86c-43fa-9d80-d97e061ae191",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaaf5c1-becb-4c1a-9a63-9eaf85bb0e66",
   "metadata": {},
   "source": [
    "Create ORC Hive Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aef97b-edbd-43c7-aaef-3011b6664307",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "create table joined_orc\n",
    "(\n",
    "    title string,\n",
    "    genres string, \n",
    "    movieId int, \n",
    "    userId int,  \n",
    "    rating float,\n",
    "    ratingTimestamp string,\n",
    "    tag string, \n",
    "    tagTimestamp string \n",
    ")\n",
    "stored as ORC\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1e178-ba7c-438c-9409-9ddd857d7611",
   "metadata": {},
   "source": [
    "Create PARQUET Hive Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fee4cb-c428-4220-b88c-c06ad989825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "create table joined_parquet\n",
    "(\n",
    "    title string,\n",
    "    genres string, \n",
    "    movieId int, \n",
    "    userId int,  \n",
    "    rating float,\n",
    "    ratingTimestamp string,\n",
    "    tag string, \n",
    "    tagTimestamp string \n",
    ")\n",
    "stored as PARQUET\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd13db7-ca76-44c0-a207-a7950034f096",
   "metadata": {},
   "source": [
    "Let’s see if the tables have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c55221-aecf-47fe-8b0c-e7f124d62d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ebd9a-dfdc-4e5e-a4a6-c05f63880d46",
   "metadata": {},
   "source": [
    "They are there. Now, let’s insert dataframe into the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f6e5b-03ba-4bae-ae99-0d5d2a87f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.write.insertInto('joined_orc')\n",
    "joined.write.insertInto('joined_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbf2d0-fb5d-4763-9262-a4f9f54beb29",
   "metadata": {},
   "source": [
    "Finally, let’s check if the data has been inserted into the Hive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea5f8-519c-4ccd-9e6a-b297918bc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    'select title, genres, rating from joined_orc order by rating desc limit 5'\n",
    ").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c709a-0778-45a8-8abc-465f1abd2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    'select title, genres, rating  from joined_parquet order by rating desc limit 5'\n",
    ").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c47fb0-c9bc-49e7-abf7-6746ee074155",
   "metadata": {},
   "source": [
    "Everything looks great!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
