{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dc0a8e-45a2-4bdb-a3f6-01727fd818d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Why Spark? Because ... Pyspark\n",
    "==========\n",
    "\n",
    "![Related\n",
    "image](https://miro.medium.com/proxy/1*sQGVLk43kXJTEw1mtJRoDw.png)\n",
    "\n",
    "Hadoop was the first open source system that introduced us to the\n",
    "MapReduce paradigm of programming and Spark is the system that made it\n",
    "faster, much much faster(100x).\n",
    "\n",
    "There used to be a lot of data movement in Hadoop as it used to write\n",
    "intermediate results to the file system.\n",
    "\n",
    "This affected the speed at which you could do analysis.\n",
    "\n",
    "Spark provided us with an in-memory model, so Spark doesn’t write too\n",
    "much to the disk while working.\n",
    "\n",
    "Simply, Spark is faster than Hadoop and a lot of people use Spark now.\n",
    "\n",
    "***So without further ado let us get started.***\n",
    "\n",
    "Load Some Data\n",
    "==============\n",
    "\n",
    "The next step is to upload some data we will use to learn Spark. We will end up using multiple datasets by the end of this but let us start with something very simple.\n",
    "\n",
    "Let us add the file `shakespeare.txt` \n",
    "\n",
    "You can see that the file is loaded to `shakespeare/shakespeare.txt` location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b5a02a-00a0-4b0b-8e84-b8ca08ac710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5332\n",
      "-rw-r--r-- 1 root root 5458199 Apr 23  2020 shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "# To download the data you would use the following commands:\n",
    "#!wget -P shakespeare https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
    "#!mv shakespeare/t8.shakespeare.txt shakespeare/shakespeare.txt\n",
    "!ls -l shakespeare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28679843-7b6e-4e24-afb8-e93e470d4a79",
   "metadata": {},
   "source": [
    "\n",
    "Our First Spark Program\n",
    "=======================\n",
    "\n",
    "I like to learn by examples so let’s get done with the “Hello World” of\n",
    "Distributed computing: ***The WordCount Program.***\n",
    "\n",
    "First, we need to create a `SparkSession`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca576c3-83fd-4e11-9fd4-7ebe34b718ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"shakespeare\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\")\\\n",
    "        .config(\"spark.executor.memory\", \"1g\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.2.0\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2320319-7a0c-41cf-b1a5-2cc291a177ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json,col\n",
    "from pyspark.sql.types import *\n",
    "from os.path import abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af20be05-9a3f-47bd-a40d-8d7bac754380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute the data - Create a RDD \n",
    "lines = sc.textFile(\"shakespeare/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ac7606-d55c-4e96-b083-8369f08985aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'the',\n",
       " '100th',\n",
       " 'Etext',\n",
       " 'file',\n",
       " 'presented',\n",
       " 'by',\n",
       " 'Project',\n",
       " 'Gutenberg,',\n",
       " 'and']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'This is the 100th Etext file presented by Project Gutenberg, and'\n",
    "x.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45173d-269d-4521-9b4e-b9ada5163183",
   "metadata": {},
   "source": [
    "Now we can write our program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97d5ccfb-405a-44dc-a119-93e1b67ff085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'to' occurs 15623 times\n",
      "'thine' occurs 315 times\n",
      "'friend.' occurs 73 times\n",
      "'Tell' occurs 179 times\n",
      "'more' occurs 1608 times\n",
      "'thou' occurs 4247 times\n",
      "'Exeunt.' occurs 122 times\n",
      "'III.' occurs 141 times\n",
      "'French' occurs 127 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Distribute the data - Create a RDD \n",
    "lines = sc.textFile(\"shakespeare/shakespeare.txt\")\n",
    "\n",
    "# Create a list with all words, Create tuple (word,1), reduce by key i.e. the word\n",
    "counts = (lines.flatMap(lambda x: x.split(' '))          \n",
    "                  .map(lambda x: (x, 1))                 \n",
    "                  .reduceByKey(lambda x,y : x + y))\n",
    "\n",
    "# get the output on local\n",
    "output = counts.take(10)                    \n",
    "\n",
    "# print output\n",
    "for (word, count) in output:    \n",
    "    if word.strip() != \"\":\n",
    "        print(f\"'{word}' occurs {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b0c42-3cc2-422b-a37d-13ffd520120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(counts.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa6200-7b30-4a27-9f76-08d4c23a5004",
   "metadata": {
    "tags": []
   },
   "source": [
    "So that is a small example which counts the number of words in the\n",
    "document and prints 10 of them.\n",
    "\n",
    "And most of the work gets done in the second command.\n",
    "\n",
    "Don’t worry if you are not able to follow this yet as I still need to\n",
    "tell you about the things that make Spark work.\n",
    "\n",
    "But before we get into Spark basics, Let us refresh some of our Python\n",
    "Basics. Understanding Spark becomes a lot easier if you have used\n",
    "[functional programming with Python](https://amzn.to/2SuAtzL).\n",
    "\n",
    "For those of you who haven’t used it, below is a brief intro.\n",
    "\n",
    "A functional approach to programming in Python \n",
    "==============================================\n",
    "\n",
    "![Related\n",
    "image](https://miro.medium.com/proxy/1*nCX6bsSNUF_v2hFKgnaQIA.png)\n",
    "\n",
    "Map \n",
    "------\n",
    "\n",
    "`map` is used to map a function to an array or a\n",
    "list. Say you want to apply some function to every element in a list.\n",
    "\n",
    "You can do this by simply using a for loop but python lambda functions\n",
    "let you do this in a single line in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5796a3d8-30b9-4049-989c-2b1342a47d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want to square each term in my_list.\n",
    "squared_list = map(lambda x:x**2, my_list)\n",
    "\n",
    "print(list(squared_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da743705-cd7d-4e58-a4e1-d148bc0a8e74",
   "metadata": {},
   "source": [
    "In the above example, you could think of `map`\n",
    "as a function which takes two arguments — A function and a list.\n",
    "\n",
    "It then applies the function to every element of the list.\n",
    "\n",
    "What lambda allows you to do is write an inline function. In here the\n",
    "part `lambda x:x**2` defines a function that takes x as input and returns x².\n",
    "\n",
    "You could have also provided a proper function in place of lambda. For\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412935ee-84d3-4f17-b8cf-818a60e08dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want to square each term in my_list.\n",
    "squared_list = map(squared, my_list)\n",
    "\n",
    "print(list(squared_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c6386-02af-4668-bb23-9af47a9a60d7",
   "metadata": {},
   "source": [
    "The same result, but the lambda expressions make the code compact and a\n",
    "lot more readable.\n",
    "\n",
    "Filter\n",
    "---------\n",
    "\n",
    "The other function that is used extensively is the `filter` function. This function takes two arguments — A condition and the list to filter.\n",
    "\n",
    "If you want to filter your list using some condition you use\n",
    "`filter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc52c119-0d2e-4960-a77a-316c82041eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "my_list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Lets say I want only the even numbers in my list.\n",
    "filtered_list = filter(lambda x:x%2==0,my_list)\n",
    "print(list(filtered_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e439961-aca2-4a3b-8ac9-fe037194ec09",
   "metadata": {},
   "source": [
    "Reduce\n",
    "---------\n",
    "\n",
    "The next function I want to talk about is the reduce function. This\n",
    "function will be the workhorse in Spark.\n",
    "\n",
    "This function takes two arguments — a function to reduce that takes two\n",
    "arguments, and a list over which the reduce function is to be applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d432830f-6c74-4adf-8e28-8a250ca0bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "my_list = [1,2,3,4,5]\n",
    "\n",
    "# Lets say I want to sum all elements in my list.\n",
    "sum_list = functools.reduce(lambda x,y:x+y,my_list)\n",
    "print(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "905b75b2-aa27-4229-b57e-57fa2c77e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "my_list = [1,2,3,4]\n",
    "\n",
    "# Lets say I want to sum all elements in my list.\n",
    "sum_list = functools.reduce(lambda x,y:x*y,my_list)\n",
    "print(sum_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda85e8-e6b0-4ecb-b1d6-7eb732c58f0e",
   "metadata": {},
   "source": [
    "In python2 reduce used to be a part of Python, now we have to use\n",
    "`reduce` as a part of `functools`.\n",
    "\n",
    "Here the lambda function takes in two values x, y and returns their sum.\n",
    "Intuitively you can think that the reduce function works as:\n",
    "\n",
    "```\n",
    "Reduce function first sends 1,2    ; the lambda function returns 3\n",
    "Reduce function then sends 3,3     ; the lambda function returns 6\n",
    "Reduce function then sends 6,4     ; the lambda function returns 10\n",
    "Reduce function finally sends 10,5 ; the lambda function returns 15\n",
    "```\n",
    "\n",
    "A condition on the lambda function we use in reduce is that it must be:\n",
    "\n",
    "-   commutative that is a + b = b + a and\n",
    "-   associative that is (a + b) + c == a + (b + c).\n",
    "\n",
    "In the above case, we used sum which is **commutative as well as\n",
    "associative**. Other functions that we could have used: `max`**,** `min`, `*` etc.\n",
    "\n",
    "Moving Again to Spark\n",
    "=====================\n",
    "\n",
    "As we have now got the fundamentals of Python Functional Programming out\n",
    "of the way, lets again head to Spark.\n",
    "\n",
    "But first, let us delve a little bit into how spark works. Spark\n",
    "actually consists of two things a driver and workers.\n",
    "\n",
    "Workers normally do all the work and the driver makes them do that work.\n",
    "\n",
    "RDD\n",
    "---\n",
    "\n",
    "An RDD(Resilient Distributed Dataset) is a parallelized data structure\n",
    "that gets distributed across the worker nodes. They are the basic units\n",
    "of Spark programming.\n",
    "\n",
    "In our wordcount example, in the first line\n",
    "\n",
    "```py\n",
    "lines = sc.textFile(\"/FileStore/tables/shakespeare.txt\")\n",
    "```\n",
    "\n",
    "we took a text file and distributed it across worker nodes so that they\n",
    "can work on it in parallel. We could also parallelize lists using the\n",
    "function `sc.parallelize`\n",
    "\n",
    "For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df29ed4d-4b25-4991-8b08-ee6f901e839e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "new_rdd = sc.parallelize(data,4)\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fd298-2106-4ffd-a52b-ac09cd5a8e78",
   "metadata": {},
   "source": [
    "\n",
    "In Spark, we can do two different types of operations on RDD:\n",
    "Transformations and Actions.\n",
    "\n",
    "1.  **Transformations:** Create new datasets from existing RDDs\n",
    "2.  **Actions:** Mechanism to get results out of Spark\n",
    "\n",
    "Transformation Basics\n",
    "=====================\n",
    "\n",
    "![Image for\n",
    "post](https://miro.medium.com/max/2560/1*LP9yglc4UeUxDFBoTlfS9w.png)\n",
    "\n",
    "So let us say you have got your data in the form of an RDD.\n",
    "\n",
    "To requote your data is now accessible to the worker machines. You want\n",
    "to do some transformations on the data now.\n",
    "\n",
    "You may want to filter, apply some function, etc.\n",
    "\n",
    "In Spark, this is done using Transformation functions.\n",
    "\n",
    "Spark provides many transformation functions. You can see a\n",
    "comprehensive list\n",
    "[**here**](http://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations).\n",
    "Some of the main ones that I use frequently are:\n",
    "\n",
    "Map:\n",
    "-------\n",
    "\n",
    "Applies a given function to an RDD.\n",
    "\n",
    "Note that the syntax is a little bit different from Python, but it\n",
    "necessarily does the same thing. Don’t worry about `collect` yet. For now, just think of it as a function that collects the data in squared\\_rdd back to a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40b7a3c5-97c0-4e34-a5fd-b91f65e917a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data,4)\n",
    "squared_rdd = rdd.map(lambda x:x**2)\n",
    "result_list = squared_rdd.collect()\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdea963-3d2d-4e16-8ecb-24207eca041f",
   "metadata": {},
   "source": [
    "\n",
    "Filter:\n",
    "----------\n",
    "\n",
    "Again no surprises here. Takes as input a condition and keeps only those\n",
    "elements that fulfill that condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "975dae22-a5f0-4c5b-9841-e1e8a0dc3997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data,4)\n",
    "filtered_rdd = rdd.filter(lambda x:x%2!=0)\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94454f-8e26-4f71-8341-ec273930811d",
   "metadata": {},
   "source": [
    "\n",
    "distinct:\n",
    "------------\n",
    "\n",
    "Returns only distinct elements in an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce5379ad-b6ee-495a-b192-26ace2544c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10]\n",
    "rdd = sc.parallelize(data,4)\n",
    "distinct_rdd = rdd.distinct()\n",
    "sorted(distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f54f1d-e598-4005-8e43-84211dfe61b3",
   "metadata": {},
   "source": [
    "flatmap:\n",
    "-----------\n",
    "\n",
    "Similar to `map`, but each input item can be\n",
    "mapped to 0 or more output items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a73decd-7e54-4cd5-871d-1e8cc287ba09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 8, 3, 27, 4, 64]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [1,2,3,4]\n",
    "rdd = sc.parallelize(data,4)\n",
    "flat_rdd = rdd.flatMap(lambda x:[x,x**3])\n",
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae04b1-7597-4d6f-b249-a5375742a418",
   "metadata": {},
   "source": [
    "Reduce By Key:\n",
    "-----------------\n",
    "\n",
    "The parallel to the reduce in Hadoop MapReduce.\n",
    "\n",
    "Now Spark cannot provide the value if it just worked with Lists.\n",
    "\n",
    "In Spark, there is a concept of pair RDDs that makes it a lot more\n",
    "flexible. Let's assume we have a data in which we have a product, its\n",
    "category, and its selling price. We can still parallelize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8a2ed0b-f0cf-4e11-bfc5-0ece52b5d4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'Fruit', 200),\n",
       " ('Banana', 'Fruit', 24),\n",
       " ('Tomato', 'Fruit', 56),\n",
       " ('Potato', 'Vegetable', 103),\n",
       " ('Carrot', 'Vegetable', 34)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]\n",
    "rdd = sc.parallelize(data,4)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd40c7-2665-4356-88db-1a5cd94f9a7e",
   "metadata": {},
   "source": [
    "Right now our RDD `rdd` holds tuples.\n",
    "\n",
    "Now we want to find out the total sum of revenue that we got from each\n",
    "category.\n",
    "\n",
    "To do that we have to transform our `rdd` to a\n",
    "pair rdd so that it only contains key-value pairs/tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96966370-c0b0-4bc9-aa5c-0d2fc3ced973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 200),\n",
       " ('Fruit', 24),\n",
       " ('Fruit', 56),\n",
       " ('Vegetable', 103),\n",
       " ('Vegetable', 34)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_price_rdd = rdd.map(lambda x: (x[1],x[2]))\n",
    "category_price_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6bf994-0aa2-463d-a787-6429d3426c2f",
   "metadata": {},
   "source": [
    "Here we used the map function to get it in the format we wanted. When\n",
    "working with textfile, the RDD that gets formed has got a lot of\n",
    "strings. We use `map` to convert it into a format that we want.\n",
    "\n",
    "So now our `category_price_rdd` contains the\n",
    "product category and the price at which the product sold.\n",
    "\n",
    "Now we want to reduce on the key category and sum the prices. We can do\n",
    "this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1871a192-f4ed-4e8f-820c-2626f539ee9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 280), ('Vegetable', 137)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x+y)\n",
    "category_total_price_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1887e-1c28-4eca-b6b6-32dbeb10185b",
   "metadata": {},
   "source": [
    "Group By Key:\n",
    "----------------\n",
    "\n",
    "Similar to `reduceByKey` but does not reduces\n",
    "just puts all the elements in an iterator. For example, if we wanted to\n",
    "keep as key the category and as the value all the products we would use\n",
    "this function.\n",
    "\n",
    "Let us again use `map` to get data in the\n",
    "required form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd830e3d-cc58-4d95-a21c-b139911bb2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 'Apple'),\n",
       " ('Fruit', 'Banana'),\n",
       " ('Fruit', 'Tomato'),\n",
       " ('Vegetable', 'Potato'),\n",
       " ('Vegetable', 'Carrot')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]\n",
    "rdd = sc.parallelize(data,4)\n",
    "category_product_rdd = rdd.map(lambda x: (x[1],x[0]))\n",
    "category_product_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc2431-57d3-4639-b49d-497559d73b39",
   "metadata": {},
   "source": [
    "We then use `groupByKey` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "813bccdb-407b-45e7-a068-a67f7180156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit ['Apple', 'Tomato', 'Banana']\n",
      "Vegetable ['Potato', 'Carrot']\n"
     ]
    }
   ],
   "source": [
    "grouped_products_by_category_rdd = category_product_rdd.groupByKey()\n",
    "findata = grouped_products_by_category_rdd.collect()\n",
    "for data in findata:\n",
    "    print(data[0],list(data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb69af3-d277-4664-8e8e-97e8ca196479",
   "metadata": {},
   "source": [
    "Here the `groupByKey` function worked and it\n",
    "returned the category and the list of products in that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a917b85-2206-46e0-83c9-fa03b2d0ed57",
   "metadata": {},
   "source": [
    "Action Basics\n",
    "=============\n",
    "\n",
    "![Image for\n",
    "post](https://miro.medium.com/max/2560/1*-T8LTnsXH2AhzhbmajacXw.png)\n",
    "\n",
    "You have filtered your data, mapped some functions on it. Done your\n",
    "computation.\n",
    "\n",
    "Now you want to get the data on your local machine or save it to a file\n",
    "or show the results in the form of some graphs in excel or any\n",
    "visualization tool.\n",
    "\n",
    "You will need actions for that. A comprehensive list of actions is\n",
    "provided\n",
    "[**here**](http://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)**.**\n",
    "\n",
    "Some of the most common actions that I tend to use are:\n",
    "\n",
    "collect:\n",
    "-----------\n",
    "\n",
    "We have already used this action many times. It takes the whole RDD and\n",
    "brings it back to the driver program.\n",
    "\n",
    "reduce:\n",
    "----------\n",
    "\n",
    "Aggregate the elements of the dataset using a function func (which takes\n",
    "two arguments and returns one). The function should be commutative and\n",
    "associative so that it can be computed correctly in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eaf53ac5-38eb-4401-a9a3-e5b7c13e97c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0f506-c4c5-46b1-9528-b4218033a783",
   "metadata": {},
   "source": [
    "take:\n",
    "--------\n",
    "\n",
    "Sometimes you will need to see what your RDD contains without getting\n",
    "all the elements in memory itself. `take`\n",
    "returns a list with the first n elements of the RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6e24ec0-fd21-43ef-9772-ee2a23095c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13871ac-2341-444a-b3d1-abe716db7828",
   "metadata": {},
   "source": [
    "\n",
    "takeOrdered:\n",
    "---------------\n",
    "\n",
    "`takeOrdered` returns the first n elements of\n",
    "the RDD using either their natural order or a custom comparator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee2cc187-80f1-4ff5-a881-617e78faa0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 12, 5]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([5,3,12,23])\n",
    "\n",
    "# descending order\n",
    "rdd.takeOrdered(3,lambda s:-1*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be4537b0-2dc2-4c06-b0b4-db757e5c80e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 344), (3, 34), (23, 29)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(5,23),(3,34),(12,344),(23,29)])\n",
    "\n",
    "# descending order\n",
    "rdd.takeOrdered(3,lambda s:-1*s[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64648f-265a-41e4-b5ee-5677a71d55ba",
   "metadata": {},
   "source": [
    "We have our basics covered finally. Let us get back to our wordcount\n",
    "example\n",
    "\n",
    "Understanding The WordCount Example\n",
    "===================================\n",
    "\n",
    "![Image for post](https://miro.medium.com/max/10368/0*wcxaKwNMIiEsGmW2)\n",
    "\n",
    "Now we sort of understand the transformations and the actions provided\n",
    "to us by Spark.\n",
    "\n",
    "It should not be difficult to understand the wordcount program now. Let\n",
    "us go through the program line by line.\n",
    "\n",
    "The first line creates an RDD and distributes it to the workers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54698698-50c0-4fa7-9d25-112e10317bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"shakespeare/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2afc0-2f91-4b26-8f58-c39288e842e6",
   "metadata": {},
   "source": [
    "This RDD `lines` contains a list of sentences in\n",
    "the file. You can see the rdd content using `take`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae4fc4-71d8-4cc8-bdf0-c9634ec2129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cf896-f873-446b-a967-4a77a2ef1152",
   "metadata": {},
   "source": [
    "This RDD is of the form:\n",
    "\n",
    "```py\n",
    "['word1 word2 word3','word4 word3 word2']\n",
    "```\n",
    "\n",
    "This next line is actually the workhorse function in the whole script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a936adc-6cf8-4418-8dd4-431dc7ad7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (lines.flatMap(lambda x: x.split(' '))          \n",
    "                  .map(lambda x: (x, 1))                 \n",
    "                  .reduceByKey(lambda x,y : x + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a8e75-00e2-4547-9e9b-979e43f10c42",
   "metadata": {},
   "source": [
    "It contains a series of transformations that we do to the lines RDD.\n",
    "First of all, we do a `flatmap` transformation.\n",
    "\n",
    "The `flatmap` transformation takes as input the\n",
    "lines and gives words as output. So after the `flatmap` transformation, the RDD is of the form:\n",
    "\n",
    "```py\n",
    "['word1','word2','word3','word4','word3','word2']\n",
    "```\n",
    "\n",
    "Next, we do a `map` transformation on the\n",
    "`flatmap` output which converts the RDD to :\n",
    "\n",
    "```py\n",
    "[('word1',1),('word2',1),('word3',1),('word4',1),('word3',1),('word2',1)]\n",
    "```\n",
    "\n",
    "Finally, we do a `reduceByKey` transformation\n",
    "which counts the number of time each word appeared.\n",
    "\n",
    "After which the RDD approaches the final desirable form.\n",
    "\n",
    "```py\n",
    "[('word1',1),('word2',2),('word3',2),('word4',1)]\n",
    "```\n",
    "\n",
    "This next line is an action that takes the first 10 elements of the\n",
    "resulting RDD locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c187e4b-6b3f-4683-8bff-01fd54e443c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee4c21-b13c-44d8-949e-89f4611f9932",
   "metadata": {},
   "source": [
    "This line just prints the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7e6b8-7c07-47fe-ba61-e9c31cfc7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word, count) in output:                 \n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b28b3-84ea-437c-aaf2-c45cb7994323",
   "metadata": {},
   "source": [
    "And that is it for the wordcount program. Hope you understand it now.\n",
    "\n",
    "So till now, we talked about the Wordcount example and the basic\n",
    "transformations and actions that you could use in Spark. But we don’t do\n",
    "wordcount in real life.\n",
    "\n",
    "We have to work on bigger problems which are much more complex. Worry\n",
    "not! Whatever we have learned till now will let us do that and more.\n",
    "\n",
    "Spark in Action with Example\n",
    "============================\n",
    "\n",
    "![Image for post](https://miro.medium.com/max/10796/0*e94sY_GitJyJz02J)\n",
    "\n",
    "Let us work with a concrete example which takes care of some usual\n",
    "transformations.\n",
    "\n",
    "We will work on Movielens\n",
    "[ml-100k.zip](https://github.com/rudrasingh21/Data-ML-100k-/raw/master/ml-100k.zip)\n",
    "dataset which is a stable benchmark dataset. 100,000 ratings from 1000\n",
    "users on 1700 movies. Released 4/1998.\n",
    "\n",
    "Let us start by downloading the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a9010-1a6a-4d4f-b6ad-8c766f45d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the data you would use the following commands:\n",
    "!rm -rf ml-100k\n",
    "!wget -P /tmp https://github.com/rudrasingh21/Data-ML-100k-/raw/master/ml-100k.zip\n",
    "!unzip /tmp/ml-100k.zip -d .\n",
    "!ls -l ml-100k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc74117-2899-4293-8c97-367a523b92b1",
   "metadata": {},
   "source": [
    "The Movielens dataset contains a lot of files but we are going to be\n",
    "working with 3 files only:\n",
    "\n",
    "​1) **Users**: This file name is kept as `u.user`. The columns in this\n",
    "file are:\n",
    "\n",
    "```py\n",
    "['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "```\n",
    "\n",
    "​2) **Ratings**: This file name is kept as `u.data`. The columns in this\n",
    "file are:\n",
    "\n",
    "```py\n",
    "['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "```\n",
    "\n",
    "​3) **Movies**: This file name is kept as `u.item`. The columns in this\n",
    "file are:\n",
    "\n",
    "```py\n",
    "['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url', and 18 more columns.....]\n",
    "```\n",
    "\n",
    "Our business partner now comes to us and asks us to find out the ***25\n",
    "most rated movie titles*** from this data. How many times a movie has\n",
    "been rated?\n",
    "\n",
    "Let us load the data in different RDDs and see what the data contains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf5b9625-9521-4c15-ac2b-758a344426a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userRDD: ['1|24|M|technician|85711']\n",
      "ratingRDD: ['196\\t242\\t3\\t881250949']\n",
      "movieRDD: ['1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0']\n"
     ]
    }
   ],
   "source": [
    "userRDD = sc.textFile(\"ml-100k/u.user\") \n",
    "ratingRDD = sc.textFile(\"ml-100k/u.data\") \n",
    "movieRDD = sc.textFile(\"ml-100k/u.item\") \n",
    "print(\"userRDD:\",userRDD.take(1))\n",
    "print(\"ratingRDD:\",ratingRDD.take(1))\n",
    "print(\"movieRDD:\",movieRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae74c2-fbce-4592-8b87-d66f5c967c43",
   "metadata": {},
   "source": [
    "We note that to answer this question we will need to use the\n",
    "`ratingRDD`. But the `ratingRDD` does not have the movie name.\n",
    "\n",
    "So we would have to merge `movieRDD` and `ratingRDD` using `movie_id`.\n",
    "\n",
    "**How we would do that in Spark?**\n",
    "\n",
    "Below is the code. We also use a new transformation `leftOuterJoin`. Do read the docs and comments in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4646c3a-a0bb-475a-99d3-759527cecd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD_movid_rating: [('242', '3'), ('302', '3'), ('377', '1'), ('51', '2')]\n",
      "RDD_movid_title: [('1', 'Toy Story (1995)'), ('2', 'GoldenEye (1995)')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd_movid_title_rating: [('741', ('4', 'Last Supper, The (1995)'))]\n",
      "rdd_title_rating: [('Last Supper, The (1995)', 1), ('Last Supper, The (1995)', 1)]\n",
      "rdd_title_ratingcnt: [('L.A. Confidential (1997)', 297), ('Broken Arrow (1996)', 254)]\n",
      "#####################################\n",
      "25 most rated movies: [('Star Wars (1977)', 583), ('Contact (1997)', 509), ('Fargo (1996)', 508), ('Return of the Jedi (1983)', 507), ('Liar Liar (1997)', 485), ('English Patient, The (1996)', 481), ('Scream (1996)', 478), ('Toy Story (1995)', 452), ('Air Force One (1997)', 431), ('Independence Day (ID4) (1996)', 429), ('Raiders of the Lost Ark (1981)', 420), ('Godfather, The (1972)', 413), ('Pulp Fiction (1994)', 394), ('Twelve Monkeys (1995)', 392), ('Silence of the Lambs, The (1991)', 390), ('Jerry Maguire (1996)', 384), ('Chasing Amy (1997)', 379), ('Rock, The (1996)', 378), ('Empire Strikes Back, The (1980)', 367), ('Star Trek: First Contact (1996)', 365), ('Titanic (1997)', 350), ('Back to the Future (1985)', 350), ('Mission: Impossible (1996)', 344), ('Fugitive, The (1993)', 336), ('Indiana Jones and the Last Crusade (1989)', 331)]\n",
      "#####################################\n"
     ]
    }
   ],
   "source": [
    "# Create a RDD from RatingRDD that only contains the two columns of interest i.e. movie_id,rating.\n",
    "RDD_movid_rating = ratingRDD.map(lambda x : (x.split(\"\\t\")[1],x.split(\"\\t\")[2]))\n",
    "print(\"RDD_movid_rating:\",RDD_movid_rating.take(4))\n",
    "\n",
    "# Create a RDD from MovieRDD that only contains the two columns of interest i.e. movie_id,title.\n",
    "RDD_movid_title = movieRDD.map(lambda x : (x.split(\"|\")[0],x.split(\"|\")[1]))\n",
    "print(\"RDD_movid_title:\",RDD_movid_title.take(2))\n",
    "\n",
    "# merge these two pair RDDs based on movie_id. For this we will use the transformation leftOuterJoin(). See the transformation document.\n",
    "rdd_movid_title_rating = RDD_movid_rating.leftOuterJoin(RDD_movid_title)\n",
    "print(\"rdd_movid_title_rating:\",rdd_movid_title_rating.take(1))\n",
    "\n",
    "# use the RDD in previous step to create (movie,1) tuple pair RDD\n",
    "rdd_title_rating = rdd_movid_title_rating.map(lambda x: (x[1][1],1 ))\n",
    "print(\"rdd_title_rating:\",rdd_title_rating.take(2))\n",
    "\n",
    "# Use the reduceByKey transformation to reduce on the basis of movie_title\n",
    "rdd_title_ratingcnt = rdd_title_rating.reduceByKey(lambda x,y: x+y)\n",
    "print(\"rdd_title_ratingcnt:\",rdd_title_ratingcnt.take(2))\n",
    "\n",
    "# Get the final answer by using takeOrdered Transformation\n",
    "print(\"#####################################\")\n",
    "print(\"25 most rated movies:\",rdd_title_ratingcnt.takeOrdered(25,lambda x:-x[1]))\n",
    "print(\"#####################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289dee8-1548-49c0-b2db-f4014570747a",
   "metadata": {},
   "source": [
    "Star Wars is the most rated movie in the Movielens Dataset.\n",
    "\n",
    "Now we could have done all this in a single command using the below\n",
    "command but the code is a little messy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db41f26-0eea-4bae-8a23-b9e58167889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((ratingRDD.map(lambda x : (x.split(\"\\t\")[1],x.split(\"\\t\")[2]))).\n",
    "     leftOuterJoin(movieRDD.map(lambda x : (x.split(\"|\")[0],x.split(\"|\")[1])))).\n",
    "     map(lambda x: (x[1][1],1)).\n",
    "     reduceByKey(lambda x,y: x+y).\n",
    "     takeOrdered(25,lambda x:-x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbf43b-4190-492c-a628-1b18dffcd0dd",
   "metadata": {},
   "source": [
    "I did this to show that you can use chaining functions with Spark and\n",
    "you could bypass the process of variable creation.\n",
    "\n",
    "\n",
    "Let us do one more. For practice:\n",
    "\n",
    "Now we want to find the most highly rated 25 movies using the same\n",
    "dataset. We actually want only those movies which have been rated at\n",
    "least 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6314a4-2f9c-4639-a5c5-34f11b7c7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have the RDD rdd_movid_title_rating: [(u'429', (u'5', u'Day the Earth Stood Still, The (1951)'))]\n",
    "# We create an RDD that contains sum of all the ratings for a particular movie\n",
    "rdd_title_ratingsum = (rdd_movid_title_rating.\n",
    "                        map(lambda x: (x[1][1],int(x[1][0]))).\n",
    "                        reduceByKey(lambda x,y:x+y))\n",
    "                        \n",
    "print(\"rdd_title_ratingsum:\",rdd_title_ratingsum.take(2))\n",
    "# Merge this data with the RDD rdd_title_ratingcnt we created in the last step\n",
    "# And use Map function to divide ratingsum by rating count.\n",
    "rdd_title_ratingmean_rating_count = (rdd_title_ratingsum.\n",
    "                                    leftOuterJoin(rdd_title_ratingcnt).\n",
    "                                    map(lambda x:(x[0],(float(x[1][0])/x[1][1],x[1][1]))))\n",
    "                                    \n",
    "print(\"rdd_title_ratingmean_rating_count:\",rdd_title_ratingmean_rating_count.take(1))\n",
    "# We could use take ordered here only but we want to only get the movies which have count\n",
    "# of ratings more than or equal to 100 so lets filter the data RDD.\n",
    "rdd_title_rating_rating_count_gt_100 = (rdd_title_ratingmean_rating_count.\n",
    "                                        filter(lambda x: x[1][1]>=100))\n",
    "                                        \n",
    "print(\"rdd_title_rating_rating_count_gt_100:\",rdd_title_rating_rating_count_gt_100.take(1))\n",
    "# Get the final answer by using takeOrdered Transformation\n",
    "print(\"#####################################\")\n",
    "print (\"25 highly rated movies:\")\n",
    "print(rdd_title_rating_rating_count_gt_100.takeOrdered(25,lambda x:-x[1][0]))\n",
    "print(\"#####################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179f020-4042-482e-a151-9ca6352a0cb3",
   "metadata": {},
   "source": [
    "We have talked about RDDs till now as they are very powerful.\n",
    "\n",
    "You can use RDDs to work with non-relational databases too.\n",
    "\n",
    "They let you do a lot of things that you couldn’t do with SparkSQL?\n",
    "\n",
    "***Yes, you can use SQL with Spark too which I am going to talk about\n",
    "now.***\n",
    "\n",
    "Spark DataFrames\n",
    "================\n",
    "\n",
    "![Image for\n",
    "post](https://miro.medium.com/max/1234/0*_Xne4_sz6lroaINt.png)\n",
    "\n",
    "Spark has provided DataFrame API to work with relational data. Here is the\n",
    "[documentation](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html#) for the adventurous folks.\n",
    "\n",
    "Remember that in the background it still is all RDDs and that is why the\n",
    "starting part of this post focussed on RDDs.\n",
    "\n",
    "I will start with some common functionalities you will need to work with\n",
    "Spark DataFrames. Would look a lot like Pandas with some syntax changes.\n",
    "\n",
    "Reading the File\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b16aa15a-bfe5-4c78-af20-a5a35b59cd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings = spark.read.load(\"ml-100k/u.data\",format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda7347-5e62-44a6-b58b-c54d7c298071",
   "metadata": {},
   "source": [
    "Show File\n",
    "------------\n",
    "\n",
    "Here is how we can show files using Spark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c4b1d45-b831-4982-8d22-ff82c2ff620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---------+\n",
      "|_c0|_c1|_c2|      _c3|\n",
      "+---+---+---+---------+\n",
      "|196|242|  3|881250949|\n",
      "|186|302|  3|891717742|\n",
      "| 22|377|  1|878887116|\n",
      "|244| 51|  2|880606923|\n",
      "|166|346|  1|886397596|\n",
      "+---+---+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63bbe67f-f0d6-4f0a-8132-78c0858832db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- unix_timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2022d-f882-4ec5-b2f6-6cb8bf6085ca",
   "metadata": {},
   "source": [
    "Change Column names\n",
    "----------------------\n",
    "\n",
    "Good functionality. Always required. Don’t forget the `*` in front of the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4a8e27f-f9cb-437c-8df1-cce31cfb7e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    196|     242|     3|     881250949|\n",
      "|    186|     302|     3|     891717742|\n",
      "|     22|     377|     1|     878887116|\n",
      "|    244|      51|     2|     880606923|\n",
      "|    166|     346|     1|     886397596|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings.toDF(*['user_id', 'movie_id', 'rating', 'unix_timestamp'])\n",
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191ca85-880e-4205-ae3f-d22fa377eb89",
   "metadata": {},
   "source": [
    "Some Basic Stats\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03e4d07c-a41f-4f8d-8c61-6f737351569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(ratings.count()) #Row Count\n",
    "print(len(ratings.columns)) #Column Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31699001-2739-402e-845b-dd077a3976db",
   "metadata": {},
   "source": [
    "We can also see the dataframe statistics using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e20f795d-1b0b-48ec-bb2a-49e4e67f72d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           user_id|          movie_id|            rating|   unix_timestamp|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            100000|            100000|            100000|           100000|\n",
      "|   mean|         462.48475|         425.53013|           3.52986|8.8352885148862E8|\n",
      "| stddev|266.61442012750905|330.79835632558473|1.1256735991443214|5343856.189502848|\n",
      "|    min|                 1|                 1|                 1|        874724710|\n",
      "|    max|               943|              1682|                 5|        893286638|\n",
      "+-------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6644b70-3a07-4df6-8b65-3c23732bd969",
   "metadata": {},
   "source": [
    "\n",
    "Select a few columns\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "502551f7-37e5-41fc-b306-8440501cc3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|movie_id|\n",
      "+-------+--------+\n",
      "|    196|     242|\n",
      "|    186|     302|\n",
      "|     22|     377|\n",
      "|    244|      51|\n",
      "|    166|     346|\n",
      "+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.select('user_id','movie_id').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9cd1c-72ec-4839-be46-11957634460a",
   "metadata": {},
   "source": [
    "Filter\n",
    "---------\n",
    "\n",
    "Filter a dataframe using multiple conditions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "086d5afd-f091-48fd-81d6-1be44be1f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    253|     465|     5|     891628467|\n",
      "|    253|     510|     5|     891628416|\n",
      "|    253|     183|     5|     891628341|\n",
      "|    253|     483|     5|     891628122|\n",
      "|    253|     198|     5|     891628392|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.filter((ratings.rating==5) & (ratings.user_id==253)).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afce146-9bd4-4a05-8f8e-17e6d850ba8e",
   "metadata": {},
   "source": [
    "Groupby\n",
    "----------\n",
    "\n",
    "We can use groupby function with a spark dataframe too. Pretty much same\n",
    "as a pandas groupby with the exception that you will need to import\n",
    "`pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c597cda3-e8aa-43e5-86ae-ed85f0609eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+------------------+\n",
      "|user_id|count(user_id)|       avg(rating)|\n",
      "+-------+--------------+------------------+\n",
      "|    148|            65|               4.0|\n",
      "|    463|           133|2.8646616541353382|\n",
      "|    471|            31|3.3870967741935485|\n",
      "|    496|           129|3.0310077519379846|\n",
      "|    833|           267| 3.056179775280899|\n",
      "+-------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "ratings.groupBy(\"user_id\").agg(F.count(\"user_id\"),F.mean(\"rating\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce0d31-0e2d-464f-8ac8-7e0a78b063a1",
   "metadata": {},
   "source": [
    "Here we have found the count of ratings and average rating from each\n",
    "user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2e146-9aa0-4599-84a8-6bade994ac75",
   "metadata": {},
   "source": [
    "Sort\n",
    "=======\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53ccc065-94d2-4e16-800d-3eb4c7b4855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|      1|      33|     4|     878542699|\n",
      "|      1|     202|     5|     875072442|\n",
      "|      1|     160|     4|     875072547|\n",
      "|      1|      61|     4|     878542420|\n",
      "|      1|     189|     3|     888732928|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings.sort(\"user_id\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed31135-786a-4c68-bd5f-64935372b4d6",
   "metadata": {},
   "source": [
    "We can also do a descending sort using `F.desc`\n",
    "function as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8fdb292-36a0-4c24-861a-f2767ee4eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    943|     570|     1|     888640125|\n",
      "|    943|     186|     5|     888639478|\n",
      "|    943|     232|     4|     888639867|\n",
      "|    943|      58|     4|     888639118|\n",
      "|    943|    1067|     2|     875501756|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# descending Sort\n",
    "from pyspark.sql import functions as F\n",
    "ratings.sort(F.desc(\"user_id\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06fcd3e-61e9-4da0-bf02-23c490dcaba9",
   "metadata": {},
   "source": [
    "Joins/Merging with Spark Dataframes\n",
    "===================================\n",
    "\n",
    "We can use SQL with dataframes and thus we can merge dataframes using SQL.\n",
    "\n",
    "Let us try to run some SQL on Ratings.\n",
    "\n",
    "We first register the ratings df to a temporary table ratings\\_table on\n",
    "which we can run sql operations.\n",
    "\n",
    "As you can see the result of the SQL select statement is again a Spark\n",
    "Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbbf6633-c486-4b16-b655-213dbe49221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|\n",
      "+-------+--------+------+--------------+\n",
      "|    196|     242|     3|     881250949|\n",
      "|    186|     302|     3|     891717742|\n",
      "|     22|     377|     1|     878887116|\n",
      "|    244|      51|     2|     880606923|\n",
      "|    166|     346|     1|     886397596|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.registerTempTable('ratings_table')\n",
    "newDF = spark.sql('select * from ratings_table where rating > 4')\n",
    "newDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c90cb9-04f3-4528-9abc-d8938b172307",
   "metadata": {},
   "source": [
    "Let us now add one more Spark Dataframe to the mix to see if we can use\n",
    "join using the SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfd591a6-5d2c-42a5-ad41-75407fc8fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------+------------------+--------------------+-------+------+---------+----------+--------+------+-----+-----------+-----+-------+---------+------+-------+-------+-------+------+--------+---+-------+\n",
      "|movie_id|      movie_title|release_date|video_release_date|            IMDb_URL|unknown|Action|Adventure|Animation |Children|Comedy|Crime|Documentary|Drama|Fantasy|Film_Noir|Horror|Musical|Mystery|Romance|Sci_Fi|Thriller|War|Western|\n",
      "+--------+-----------------+------------+------------------+--------------------+-------+------+---------+----------+--------+------+-----+-----------+-----+-------+---------+------+-------+-------+-------+------+--------+---+-------+\n",
      "|       1| Toy Story (1995)| 01-Jan-1995|              null|http://us.imdb.co...|      0|     0|        0|         1|       1|     1|    0|          0|    0|      0|        0|     0|      0|      0|      0|     0|       0|  0|      0|\n",
      "|       2| GoldenEye (1995)| 01-Jan-1995|              null|http://us.imdb.co...|      0|     1|        1|         0|       0|     0|    0|          0|    0|      0|        0|     0|      0|      0|      0|     0|       1|  0|      0|\n",
      "|       3|Four Rooms (1995)| 01-Jan-1995|              null|http://us.imdb.co...|      0|     0|        0|         0|       0|     0|    0|          0|    0|      0|        0|     0|      0|      0|      0|     0|       1|  0|      0|\n",
      "|       4|Get Shorty (1995)| 01-Jan-1995|              null|http://us.imdb.co...|      0|     1|        0|         0|       0|     1|    0|          0|    1|      0|        0|     0|      0|      0|      0|     0|       0|  0|      0|\n",
      "|       5|   Copycat (1995)| 01-Jan-1995|              null|http://us.imdb.co...|      0|     0|        0|         0|       0|     0|    1|          0|    1|      0|        0|     0|      0|      0|      0|     0|       1|  0|      0|\n",
      "+--------+-----------------+------------+------------------+--------------------+-------+------+---------+----------+--------+------+-----+-----------+-----+-------+---------+------+-------+-------+-------+------+--------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get one more dataframe to join\n",
    "movies = spark.read.load(\"ml-100k/u.item\",format=\"csv\", sep=\"|\", inferSchema=\"true\", header=\"false\")\n",
    "\n",
    "# change column names\n",
    "movies = movies.toDF(*[\"movie_id\",\"movie_title\",\"release_date\",\"video_release_date\",\"IMDb_URL\",\"unknown\",\"Action\",\"Adventure\",\"Animation \",\"Children\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film_Noir\",\"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci_Fi\",\"Thriller\",\"War\",\"Western\"])\n",
    "\n",
    "# display\n",
    "movies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc85d5e-4444-4b72-a73c-d12d24cd93fc",
   "metadata": {},
   "source": [
    "Now let us try joining the tables on movie\\_id to get the name of the\n",
    "movie in the ratings table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ccf44908-d90d-46c3-b0f6-362db102c41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+--------------------+\n",
      "|user_id|movie_id|rating|unix_timestamp|         movie_title|\n",
      "+-------+--------+------+--------------+--------------------+\n",
      "|    196|     242|     3|     881250949|        Kolya (1996)|\n",
      "|    186|     302|     3|     891717742|L.A. Confidential...|\n",
      "|     22|     377|     1|     878887116| Heavyweights (1994)|\n",
      "|    244|      51|     2|     880606923|Legends of the Fa...|\n",
      "|    166|     346|     1|     886397596| Jackie Brown (1997)|\n",
      "+-------+--------+------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.registerTempTable('movies_table')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "    ratings_table.*,\n",
    "    movies_table.movie_title \n",
    "from ratings_table \n",
    "left join movies_table \n",
    "    on movies_table.movie_id = ratings_table.movie_id\n",
    "\"\"\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d2b35-0a0b-4304-ba8a-f80a4ef4f55c",
   "metadata": {},
   "source": [
    "Let us try to do what we were doing earlier with the RDDs. Finding the\n",
    "top 25 most rated movies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c452c60e-cd0b-4fe0-8355-655831881960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|movie_id|         movie_title|num_ratings|\n",
      "+--------+--------------------+-----------+\n",
      "|      50|    Star Wars (1977)|        583|\n",
      "|     258|      Contact (1997)|        509|\n",
      "|     100|        Fargo (1996)|        508|\n",
      "|     181|Return of the Jed...|        507|\n",
      "|     294|    Liar Liar (1997)|        485|\n",
      "+--------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mostrateddf = spark.sql(\"\"\"\n",
    "select \n",
    "    movie_id,\n",
    "    movie_title, \n",
    "    count(user_id) as num_ratings \n",
    "from (\n",
    "    select \n",
    "        ratings_table.*,\n",
    "        movies_table.movie_title \n",
    "    from ratings_table \n",
    "    left join movies_table \n",
    "        on movies_table.movie_id = ratings_table.movie_id\n",
    "    )A \n",
    "group by movie_id, movie_title \n",
    "order by num_ratings desc \n",
    "\"\"\")\n",
    "\n",
    "mostrateddf.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa96d7-24d1-41fa-a663-ef5fa2b0bcbb",
   "metadata": {},
   "source": [
    "And finding the top 25 highest rated movies having more than 100 votes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f88217c9-d9c6-4d03-bc2a-d145ba17eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+-----------------+-----------+\n",
      "|movie_id|movie_title                     |avg_rating       |num_ratings|\n",
      "+--------+--------------------------------+-----------------+-----------+\n",
      "|408     |Close Shave, A (1995)           |4.491071428571429|112        |\n",
      "|318     |Schindler's List (1993)         |4.466442953020135|298        |\n",
      "|169     |Wrong Trousers, The (1993)      |4.466101694915254|118        |\n",
      "|483     |Casablanca (1942)               |4.45679012345679 |243        |\n",
      "|64      |Shawshank Redemption, The (1994)|4.445229681978798|283        |\n",
      "+--------+--------------------------------+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "highrateddf = spark.sql(\"\"\"\n",
    "select \n",
    "    movie_id,\n",
    "    movie_title, \n",
    "    avg(rating) as avg_rating,\n",
    "    count(movie_id) as num_ratings \n",
    "from (\n",
    "    select \n",
    "        ratings_table.*,\n",
    "        movies_table.movie_title \n",
    "    from ratings_table \n",
    "    left join movies_table \n",
    "        on movies_table.movie_id = ratings_table.movie_id\n",
    "    )A \n",
    "group by movie_id, movie_title \n",
    "having num_ratings>100 \n",
    "order by avg_rating desc \n",
    "\"\"\")\n",
    "\n",
    "highrateddf.show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7e842-faf1-4e9c-b2ac-4971feea8eba",
   "metadata": {},
   "source": [
    "Converting from Spark Dataframe to RDD and vice versa:\n",
    "======================================================\n",
    "\n",
    "Sometimes you may want to convert to RDD from a spark Dataframe or vice\n",
    "versa so that you can have the best of both worlds.\n",
    "\n",
    "To convert from DF to RDD, you can simply do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a5713d1-93a6-4b8e-8dc7-4b5b33b30a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(movie_id=408, movie_title='Close Shave, A (1995)', avg_rating=4.491071428571429, num_ratings=112),\n",
       " Row(movie_id=318, movie_title=\"Schindler's List (1993)\", avg_rating=4.466442953020135, num_ratings=298)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highratedrdd = highrateddf.rdd\n",
    "highratedrdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4324d-f1d1-48c8-bb67-362bbafb4364",
   "metadata": {},
   "source": [
    "To go from an RDD to a dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d81b90d0-267b-4d10-b715-c31579f32938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A|  1|\n",
      "|   B|  2|\n",
      "|   C|  3|\n",
      "|   D|  4|\n",
      "+----+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   D|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# creating a RDD first\n",
    "data = [('A',1),('B',2),('C',3),('D',4)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# map the schema using Row.\n",
    "rdd_rows = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "\n",
    "# Convert the rdd to Dataframe\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "df.registerTempTable('people')\n",
    "spark.sql('select * from people where age > 3').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978e77e-5056-49dc-9490-0dfd2bb60123",
   "metadata": {},
   "source": [
    "RDD provides you with ***more control*** at the cost of time and coding\n",
    "effort. While Dataframes provide you with ***familiar coding***\n",
    "platform. And now you can move back and forth between these two.\n",
    "\n",
    "Conclusion\n",
    "==========\n",
    "\n",
    "![Image for post](https://miro.medium.com/max/10336/0*TK-uI698Vdxjh5kL)\n",
    "\n",
    "This was long and congratulations if you reached the end.\n",
    "\n",
    "[Spark](https://spark.apache.org/) has provided us with an interface where\n",
    "we could use transformations and actions on our data. Spark also has the\n",
    "Dataframe API to ease the transition to Big Data.\n",
    "\n",
    "Hopefully, I’ve covered the basics well enough to pique your interest\n",
    "and help you get started with Spark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
