{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471e4da2-6b6f-485e-90fc-e3f375f7ee35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tweets Visualization\n",
    "\n",
    "We will see how to use Spark Structured Streaming with Kafka to visualize hashtags counts. \n",
    "\n",
    "First we need to get a SparkSession with Kafka support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('tweets-dataviz')\n",
    "         .config(\"spark.sql.streaming.schemaInference\", True) #Stream dataframe infers schema\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1')\n",
    "         .config('spark.sql.shuffle.partitions', '1')\n",
    "         .config('spark.sql.adaptive.enabled', 'false')\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58070278-909c-4864-9c8d-52be4a498275",
   "metadata": {},
   "source": [
    "Then we need to start our streaming application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25558fc-22ae-4020-b25d-32889c4b2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "topic = \"savas\"\n",
    "checkpoint = f\"file:///tmp/checkpoint_{topic}_1\"\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from Kafka topic\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "lines = df \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\").alias(\"value\") \\\n",
    "    .withColumn('value', regexp_replace('value', '\\n', ''))\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count for hashtags\n",
    "wordCounts = words \\\n",
    "    .filter(\"word like '%#%'\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False)\n",
    "\n",
    "# Start running the query\n",
    "queryStream = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint) \\\n",
    "    .queryName(\"wiki_changes\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b26e5e-dacf-4650-9cb0-a258d16a25a9",
   "metadata": {},
   "source": [
    "Finally we can start our visualization. Once it is started, the user can send tweets from his account and and we can see the hashtags count here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588d130-bf2d-4bfa-bb99-bf5839fcab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rc('font', family='DejaVu Sans')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "try:\n",
    "    i=1\n",
    "    while True:\n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"**********************\")\n",
    "        print(\"General Info\")\n",
    "        print(\"**********************\")\n",
    "        print(\"Run:{}\".format(i))\n",
    "        if (queryStream.lastProgress):\n",
    "            \n",
    "            print(\"Stream timestamp:{}\".format(queryStream.lastProgress[\"timestamp\"]))\n",
    "            if queryStream.lastProgress[\"stateOperators\"]:\n",
    "                # print(\"Watermark:{}\".format(queryStream.lastProgress[\"eventTime\"][\"watermark\"]))\n",
    "                print(\"Total Rows:{}\".format(queryStream.lastProgress[\"stateOperators\"][0][\"numRowsTotal\"]))\n",
    "                print(\"Updated Rows:{}\".format(queryStream.lastProgress[\"stateOperators\"][0][\"numRowsUpdated\"]))\n",
    "                print(\"Memory used MB:{}\".format((queryStream.lastProgress[\"stateOperators\"][0][\"memoryUsedBytes\"]) * 0.000001))\n",
    "            \n",
    "        df = spark.sql(\n",
    "                \"\"\"\n",
    "                    select word as hashtag, count\n",
    "                    from wiki_changes\n",
    "                    order by count desc\n",
    "                    limit 10\n",
    "                \"\"\"\n",
    "        ).toPandas()\n",
    "\n",
    "        # Plot the total crashes\n",
    "        sns.set_color_codes(\"muted\")\n",
    "\n",
    "        # Initialize the matplotlib figure\n",
    "        plt.figure(figsize=(8,6))\n",
    "\n",
    "        print(\"***********************\")\n",
    "        print(\"Graph - Top 10 hashtags\")\n",
    "        print(\"***********************\")\n",
    "        try:\n",
    "            # Barplot\n",
    "            sns.barplot(x=\"count\", y=\"hashtag\", data=df)\n",
    "\n",
    "            # Show barplot\n",
    "            plt.show()\n",
    "        except ValueError:\n",
    "            # If Dataframe is empty, pass\n",
    "            pass\n",
    "\n",
    "        print(\"***********************\")\n",
    "        print(\"Table - Top 10 hashtags\")\n",
    "        print(\"***********************\")\n",
    "        display(df)\n",
    "        \n",
    "\n",
    "        sleep(10)\n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"process interrupted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08ee51-3d2f-4aee-ac5c-8f85e4825089",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test\n",
    "\n",
    "You can test the visualization by producing directly into the topic from the terminal:\n",
    "\n",
    "\n",
    ">docker exec -it broker /bin/sh                                          \n",
    "\n",
    ">sh-4.4$ kafka-console-producer --topic savas --bootstrap-server broker:29092\n",
    "\n",
    ">this is from #console\n",
    "\n",
    ">I love #console #console #console #console\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1072910-ae09-407d-b842-dae4ecdedb7d",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dec7f0-036e-4ae5-ba2e-b4f21b2a84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check active streams\n",
    "for s in spark.streams.active:\n",
    "    print(\"ID:{} | NAME:{}\".format(s.id, s.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093db368-25e6-4cd8-9fee-0b8b61a94a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop stream\n",
    "queryStream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c47fb0-c9bc-49e7-abf7-6746ee074155",
   "metadata": {},
   "source": [
    "Everything looks great!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
